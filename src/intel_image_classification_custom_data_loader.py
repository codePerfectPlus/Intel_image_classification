# -*- coding: utf-8 -*-
"""intel-image-classification-custom-data-loader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D__t5BtPv03FmHOZjJ4NyGwfe0xer4nR
"""

""" Training scrip(t in Tensorflow """
import os
import numpy as np
import pandas as pd
from PIL import Image
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split
import tensorflow as tf
from tensorflow.keras import layers
from tensorflow.keras.applications.vgg16 import VGG16
from tensorflow.keras.applications.vgg16 import preprocess_input

train_dir = "../input/intel-image-classification/seg_train/seg_train"
test_dir = "../input/intel-image-classification/seg_test/seg_test"
img_size = (150, 150)
img_shape = (150, 150, 3)
batch_size = 10
num_classes = len(os.listdir(train_dir))
idx_to_name = os.listdir(train_dir)
name_to_idx = dict([(v, k) for k, v in enumerate(idx_to_name)])

name_to_idx

def data_to_df(data_dir, subset=None):
    df = pd.DataFrame()
    filenames = []
    labels = []
    
    for dataset in os.listdir(data_dir):
        img_list = os.listdir(os.path.join(data_dir, dataset))
        label = name_to_idx[dataset]
        
        for image in img_list:
            filenames.append(os.path.join(data_dir, dataset, image))
            labels.append(label)
        
    df["filenames"] = filenames
    df["labels"] = labels
    
    if subset == "train":
        train_df, val_df = train_test_split(df, train_size=0.8, shuffle=True,
                                            random_state=10)
        return train_df, val_df
    
    return df

print("Converting data directory to dataframe")
train_df, val_df = data_to_df(train_dir, subset="train")

class CustomDataGenerator:
    ''' Custom DataGenerator to load img 
    
    Arguments:
        data_frame = pandas data frame in filenames and labels format
        batch_size = divide data in batches
        shuffle = shuffle data before loading
        img_shape = image shape in (h, w, d) format
        augmentation = data augmentation to make model rebust to overfitting
    
    Output:
        Img: numpy array of image
        label : output label for image
    '''
    def __init__(self, data_frame, batch_size=10, shuffle_data=True, img_shape=None, augmentation=True):
        self.data_frame = data_frame
        self.train_len = self.data_frame.shape[0]
        self.batch_size = batch_size
        self.shuffle_data = shuffle_data
        self.img_shape = img_shape
    
    def __len__(self):
        return len(self.train_len/self.batch_size)

    def preprocessing(self, filename, label):
        """ converting filenames into numpy array and applying image preprocessing """
        img = tf.keras.preprocessing.image.load_img(filename)
        img = tf.keras.preprocessing.image.img_to_array(img)
        img = np.resize(img, self.img_shape)
        img = preprocess_input(img)
        img = tf.keras.preprocessing.image.random_shift(img, 0.2, 0.3)
        label = tf.keras.utils.to_categorical(label, num_classes)
        return img, label

    def generate(self):
        ''' Generator function to yield img and label '''
        num_samples = self.data_frame.shape[0]
        while True:
            if self.shuffle_data:
                self.data_frame = shuffle(self.data_frame)

            for offset in range(0, num_samples, self.batch_size):
                batch_samples = self.data_frame[offset:offset+self.batch_size]
                
                filenames = batch_samples["filenames"]
                labels = batch_samples["labels"]
                X_train = []
                y_train = []
                
                for filename, label in zip(filenames, labels):
                    img, label = self.preprocessing(filename, label)

                    X_train.append(img)
                    y_train.append(label)

                X_train = np.array(X_train)
                y_train = np.array(y_train)

                yield X_train, y_train

print("creating train and validation data")
train_data = CustomDataGenerator(train_df, 
                                 batch_size=batch_size, 
                                 img_shape=img_shape).generate()
val_data = CustomDataGenerator(val_df, 
                               batch_size=10, img_shape=img_shape).generate()

x, y = next(train_data)
print(x.shape)

base_model = VGG16(weights="imagenet", include_top=False, input_shape=(64, 64, 3))
base_model.trainable= True

inputs = layers.Input(shape=img_shape)
out = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv1')(inputs)
out = layers.Conv2D(64, (3, 3), activation='relu', padding='same', name='block1_conv2')(out)
out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block1_pool')(out)

out = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv1')(out)
out = layers.Conv2D(128, (3, 3), activation='relu', padding='same', name='block2_conv2')(out)
out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block2_pool')(out)

out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv1')(out)
out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv2')(out)
out = layers.Conv2D(256, (3, 3), activation='relu', padding='same', name='block3_conv3')(out)
out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block3_pool')(out)

out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv1')(out)
out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv2')(out)
out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block4_conv3')(out)
out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block4_pool')(out)

out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv1')(out)
out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv2')(out)
out = layers.Conv2D(512, (3, 3), activation='relu', padding='same', name='block5_conv3')(out)
out = layers.MaxPooling2D((2, 2), strides=(2, 2), name='block5_pool')(out)

out = layers.GlobalAveragePooling2D()(out)
out = layers.Dense(128, activation="relu")(out)
out = layers.Dropout(0.5)(out)
outputs = layers.Dense(6, activation="softmax")(out)

model = tf.keras.Model(inputs, outputs, name="VGG16")

model.summary()

model.compile(optimizer = 'adam', 
              loss = tf.keras.losses.CategoricalCrossentropy(), 
              metrics=['accuracy'])

modelcheckpoint = tf.keras.callbacks.ModelCheckpoint(
    "tmp",
    monitor="val_loss",
    verbose=0,
    save_best_only=True,
    save_weights_only=False,
    mode="auto",
    save_freq="epoch",
    options=None)

early_stopping = tf.keras.callbacks.EarlyStopping(
    monitor="val_loss",
    min_delta=0,
    patience=5,
    verbose=0,
    mode="auto",
    baseline=None,
    restore_best_weights=False,
)

csv_logger = tf.keras.callbacks.CSVLogger("log.csv", separator=",", append=False)

callbacks = [modelcheckpoint, early_stopping, csv_logger]

history = model.fit(train_data,
                    batch_size=batch_size,
                    epochs=1,
                    steps_per_epoch=len(train_df)//batch_size,
                    validation_data=val_data,
                    callbacks= callbacks)

test_df = data_to_df(test_dir)
test_data = CustomDataGenerator(test_df, 
                                 batch_size=batch_size, 
                                 img_shape=img_shape).generate()

model.save("saved_model")
res = model.evaluate(test_data)

print(res)